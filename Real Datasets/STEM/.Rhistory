setwd("~/Desktop/Final Code/Real Datasets/STEM")
load("Error_Penalized_Defmb.Rdata")
#For each method and find best choice of mtry, nodesize
FindBest=function(nfolds, OOBError){ #For loss func, 1=Log, 2=MSE, 3=Sph,4=Boosting Loss, 5=AUC #should usually use log
Inds=array(NA, dim=c( nfolds, 4,2))
for (fold in 1:nfolds){
for(tech in 1:4)  {
Inds[fold,tech,] = which(OOBError[fold,tech,,] == min(OOBError[fold,tech,,]), arr.ind=TRUE)[1,] #1 in 3rd index is for log loss, [1,]pulls off first in case of tie
}
}
return(Inds)
}
#Get rid of first dim since there is only 1 rep and set last to default mb
OOBErrors=OOBError[1,,,,,,]
PEAll=PredError[1,,,,,,]
#Focus on log Loss
OOBErrors=OOBErrors[,,1,,,]
PEAll=PEAll[,,1,,,]
nfolds=9
#Values of alpha to consider
alphavec=c(1,1.5,2,3,4,5,10,20,50,100,500,1000)
Parameters=array(NA, dim=c(9,4,2,length(alphavec)))   #dims are nfolds, #tech, #tuning par, #alpha
for(a in 1:length(alphavec)){
Parameters[,,,a]=FindBest(nfolds,OOBErrors[,,,,a])
}
#dimensions are 1-fold, 2-technique, 3-paramter (1=mtry, 2=nodesize), 4=alpha
#Parameters[1,,2,] gives indices of best nodesize parameter for first rep (rows are technique, cols are alpha)
#These are logloss for each nodesize using the best mtry, minbucket comination from CV for each of 9 folds
CVPredictionErrorbynodesize=array(NA, dim=c(nfolds,4,16,length(alphavec)))
for (a in 1:length(alphavec)){
ParInds=Parameters[,,,a]
for (fold in 1:nfolds){
for(tech in 1:4)  {
for(ndsize in 1:16)
CVPredictionErrorbynodesize[fold, tech, ,a]=PEAll[fold,tech,ParInds[fold,tech,][1],,a]
}
}
}
#average across folds
CVPEN=apply(CVPredictionErrorbynodesize,c(2,3,4),mean)
#Find prediction error from test set correponding to what would have been chosen using cross validation
CVPredictionError=array(NA, dim=c(nfolds,4,length(alphavec)))
for (a in 1:length(alphavec)){
ParInds=Parameters[,,,a]
for (fold in 1:nfolds){
for(tech in 1:4)  {
CVPredictionError[fold, tech,a]=PEAll[fold,tech,ParInds[fold,tech,][1],ParInds[fold,tech,][2],a]
}
}
}
colMeans(CVPredictionError[,,1])
#df=subset(df, Method!=c("RFc"))
theme_set(theme_gray(base_size = 18))
#Plot average estimated probability against nodesize for each technique
qplot(data=df2, x=Alpha, y=Error, colour=Method, xlab="Penalty Ratio", ylab="Error")
qplot(data=df2, x=Alpha, y=Error, colour=Method,xlim=c(0,1000),ylim=c(1,1.01), xlab="Penalty Ratio", ylab="Error")
ndsize=c(1,5,10,25,50,75,100,150,200,250,300,400,500, 1000, 2000, 3000)
NSInd=Parameters[,,2,a]  #dims are fold x technique
#Convert nodesize indices to nodesizes
NodesizeMat=array(NA, dim=c(nfolds,4,length(alphavec)))
for (a in 1:length(alphavec)){
NSInd=Parameters[,,2,a]  #dims are fold x technique
for (i in 1:nrow(NSInd)){
for (j in 1:ncol(NSInd)){
NodesizeMat[i,j,a]=ndsize[NSInd[i,j]]
}
}
}
NodesizeMat
PenError=MeanPE[2:4,]
NodesizeMat
round(apply(NodesizeMat, c(2,3), mean),1)
MeanPE
#Calculate mean prediction error for each alpha and each technique, store in dataframe df2
MeanPE=apply(CVPredictionError, c(2,3), mean)
MinPE=apply(MeanPE, 2,min)
ER=t(apply(MeanPE, 1, function(x) x/MinPE))
Mthd=c(rep(c("RF-RV", "RF-RP", "CF-WTP", "CF-TP"),length(alphavec)))
df2=data.frame(rep(alphavec, each=4), Mthd,c(ER))
names(df2)=c("Alpha", "Method", "Error")
#df=subset(df, Method!=c("RFc"))
theme_set(theme_gray(base_size = 18))
#Plot average estimated probability against nodesize for each technique
qplot(data=df2, x=Alpha, y=Error, colour=Method, xlab="Penalty Ratio", ylab="Error")
qplot(data=df2, x=Alpha, y=Error, colour=Method,xlim=c(0,1000),ylim=c(1,1.01), xlab="Penalty Ratio", ylab="Error")
ndsize=c(1,5,10,25,50,75,100,150,200,250,300,400,500, 1000, 2000, 3000)
#Look at nodesizes selected through CV
NSInd=Parameters[,,2,a]  #dims are fold x technique
#Convert nodesize indices to nodesizes
NodesizeMat=array(NA, dim=c(nfolds,4,length(alphavec)))
for (a in 1:length(alphavec)){
NSInd=Parameters[,,2,a]  #dims are fold x technique
for (i in 1:nrow(NSInd)){
for (j in 1:ncol(NSInd)){
NodesizeMat[i,j,a]=ndsize[NSInd[i,j]]
}
}
}
NodesizeMat
round(apply(NodesizeMat, c(2,3), mean),1)
MeanPE
